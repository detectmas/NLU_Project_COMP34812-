{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc3iytHdUUmA",
        "outputId": "65a0f0b8-d61f-4c72-e24d-fc2a1e9c3741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import json\n",
        "import glob\n",
        "from urllib import request\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKXcFRfYUX5M",
        "outputId": "582782a6-76a2-425c-e64b-0c72ac0171c9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "input_file_name = 'ED_trial.csv'\n",
        "\n",
        "test_data = pd.read_csv(input_file_name)"
      ],
      "metadata": {
        "id": "WdGxFVRRVChx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile(r'[/(){}\\[\\]\\|@`\\']')\n",
        "REMOVE_BEGINNING_EVIDENCES_RE = re.compile(r'\\b[0-9]{1,}')\n",
        "REMOVE_REF_EVIDENCE_RE = re.compile(r'\\[REF|ref\\]?\\.?')\n",
        "SPLIT_COMPOUND_RE = re.compile(r'\\w+(-)\\w+')\n",
        "GOOD_SYMBOLS_RE = re.compile(r'[^0-9a-z \\.;]')\n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "def remove_beginning_evidence(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes the beginning evidence from the text.\n",
        "    Example:\n",
        "    Input: '1. I really like New York city'\n",
        "    Output: 'I really like New York city'\n",
        "    \"\"\"\n",
        "    return REMOVE_BEGINNING_EVIDENCES_RE.sub('', text)\n",
        "\n",
        "def split_compound_words(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Splits compound words in the text.\n",
        "    Example:\n",
        "    Input: 'I really like Winston-Salem city'\n",
        "    Output: 'I really like Winston Salem city'\n",
        "    \"\"\"\n",
        "    return SPLIT_COMPOUND_RE.sub(' ', text)\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def remove_reference_markers(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove [REF] or [REF at the at of evidence texts\n",
        "    \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        return REMOVE_REF_EVIDENCE_RE.sub('', text)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "    return text.strip()\n",
        "\n",
        "PREPROCESSING_PIPELINE_CLAIM = [\n",
        "                          lower,\n",
        "                          replace_special_characters,\n",
        "                          split_compound_words,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "PREPROCESSING_PIPELINE_EVIDENCE = [\n",
        "                          remove_reference_markers,\n",
        "                          remove_beginning_evidence,\n",
        "                          lower,\n",
        "                          replace_special_characters,\n",
        "                          split_compound_words,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "\n",
        "def text_prepare(text, filter_methods):\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "\n",
        "#Preprocess the Claim and Evidence the same way that the training dataset was preprocessed on which the model was trained.\n",
        "\n",
        "print('Pre-processing text...')\n",
        "\n",
        "test_data['Claim'] = test_data['Claim'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_CLAIM))\n",
        "test_data['Evidence'] = test_data['Evidence'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_EVIDENCE))\n",
        "\n",
        "print(\"Pre-processing completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC-6atMfVRDG",
        "outputId": "1d58a8a7-3f93-4653-a0ff-e8caef149571"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-processing text...\n",
            "Pre-processing completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NotAdaptedError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class TextVectorizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        glove_url=\"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
        "        embedding_dim=100,\n",
        "        embedding_folder=\"glove\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This class parses the GloVe embeddings, the input documents are expected\n",
        "        to be in the form of a list of lists.\n",
        "        [[\"word1\", \"word2\", ...], [\"word1\", \"word2\", ...], ...]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        glove_url : The url of the GloVe embeddings.\n",
        "        embedding_dim : The dimension of the embeddings (pick one of 50, 100, 200, 300).\n",
        "        embedding_folder : folder where the embedding will be downloaded\n",
        "        \"\"\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.download_glove_if_needed(\n",
        "            glove_url=glove_url, embedding_folder=embedding_folder\n",
        "        )\n",
        "\n",
        "        # create the embeddings vocabulary\n",
        "        self.vocabulary = self.parse_glove(embedding_folder)\n",
        "\n",
        "    def download_glove_if_needed(self, glove_url, embedding_folder):\n",
        "        \"\"\"\n",
        "        Downloads the glove embeddings from the internet\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        glove_url : The url of the GloVe embeddings.\n",
        "        embedding_folder: folder where the embedding will be downloaded\n",
        "        \"\"\"\n",
        "        # create embedding folder if it does not exist\n",
        "        if not os.path.exists(embedding_folder):\n",
        "            os.makedirs(embedding_folder)\n",
        "\n",
        "        # extract the embedding if it is not extracted\n",
        "        if not glob.glob(\n",
        "            os.path.join(embedding_folder, \"**/glove*.txt\"), recursive=True\n",
        "        ):\n",
        "\n",
        "            # download the embedding if it does not exist\n",
        "            embedding_zip = os.path.join(embedding_folder, glove_url.split(\"/\")[-1])\n",
        "            print(embedding_zip)\n",
        "            if not os.path.exists(embedding_zip):\n",
        "                print(\"Downloading the GloVe embeddings...\")\n",
        "                request.urlretrieve(glove_url, embedding_zip)\n",
        "                print(\"Successful download!\")\n",
        "\n",
        "            # extract the embedding\n",
        "            print(\"Extracting the embeddings...\")\n",
        "            with zipfile.ZipFile(embedding_zip, \"r\") as zip_ref:\n",
        "                zip_ref.extractall(embedding_folder)\n",
        "                print(\"Successfully extracted the embeddings!\")\n",
        "            os.remove(embedding_zip)\n",
        "\n",
        "    def parse_glove(self, embedding_folder):\n",
        "        \"\"\"\n",
        "        Parses the GloVe embeddings from their files, filling the vocabulary.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding_folder : folder where the embedding files are stored\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dictionary representing the vocabulary from the embeddings\n",
        "        \"\"\"\n",
        "        print(\"Creating glove vocabulary...\")\n",
        "        vocabulary = {\"<pad>\": np.zeros(self.embedding_dim)}\n",
        "        embedding_file = os.path.join(\n",
        "            embedding_folder, \"glove.6B.\" + str(self.embedding_dim) + \"d.txt\"\n",
        "        )\n",
        "        print(embedding_file)\n",
        "        with open(embedding_file, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                vocabulary[word] = coefs\n",
        "        return vocabulary\n",
        "    def adapt(self, dataset, columns):\n",
        "        \"\"\"\n",
        "        Computes the OOV words for a single data split, and adds them to the vocabulary and recreate the dictionary of index encodings.\n",
        "        Then build the embedding matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : The data split (might be training set, validation set, or test set).\n",
        "        columns : The columns to be adapted.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        The embedding matrix of shape (vocabulary_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        # create a set containing words from the documents in a given data split\n",
        "        words = {word for column in columns for sentence in dataset[column] for word in nltk.word_tokenize(sentence)}\n",
        "        oov_words = words - self.vocabulary.keys()\n",
        "\n",
        "        # add the OOV words to the vocabulary giving them a random encoding\n",
        "        for word in oov_words:\n",
        "            self.vocabulary[word] = np.random.uniform(-1, 1, size=self.embedding_dim)\n",
        "\n",
        "        # create the dictionary of index encodings for the words in the embedding vocabulary (idx 0 is reserved for padding)\n",
        "        self.word_to_idx = {word: i for i, word in enumerate(self.vocabulary.keys())}\n",
        "        self.idx_to_word = {i: word for i, word in enumerate(self.vocabulary.keys())}\n",
        "\n",
        "        # the embedding matrix shape will be (vocabulary_size, embedding_dim)\n",
        "        self.embedding_matrix = np.array(list(self.vocabulary.values()))\n",
        "        print(f\"Generated embeddings for {len(oov_words)} OOV words.\")\n",
        "\n",
        "    def transform(self, dataset, columns):\n",
        "        \"\"\"\n",
        "        Transform the data into the input structure for the training. This method should be used always after the adapt method.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : The data split (might be training set, validation set, or test set).\n",
        "        columns : The columns to be transformed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Pair of docuemnts into idx sequences\n",
        "        \"\"\"\n",
        "        X_claim, X_evidence = [], []\n",
        "        for _, row in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Converting data into idx sequences...\"):\n",
        "            X_claim.append(self._transform_document_to_encoding(row[\"Claim\"]))\n",
        "            X_evidence.append(self._transform_document_to_encoding(row[\"Evidence\"]))\n",
        "        return X_claim, X_evidence\n",
        "\n",
        "\n",
        "    def _transform_document_to_encoding(self, document):\n",
        "        \"\"\"\n",
        "        Transforms a single document to a list of word encodings.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        document : The document to be transformed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List of word encodings\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return [self.word_to_idx[word] for word in nltk.word_tokenize(document)]\n",
        "        except KeyError:\n",
        "            raise NotAdaptedError(\n",
        "                f\"The whole document is not in the vocabulary. Please adapt the vocabulary first.\"\n",
        "            )"
      ],
      "metadata": {
        "id": "A7LOH_HJVz8d"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input(dataset, columns, vectorizer, is_training=False, max_tokens=None):\n",
        "    \"\"\"\n",
        "    Convert the text into a given dataset split into idx sequeces.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : The data split (might be training set, validation set, or test set).\n",
        "    columns : The columns to be converted.\n",
        "    vectorizer : The vectorizer to be used.\n",
        "    is_training : Whether input texts are from the training split or not\n",
        "    max_tokens : The max token sequence previously computed with\n",
        "\n",
        "    Return\n",
        "    ---------\n",
        "    X_claim: a numpy array of shape (num_documents, max_tokens) representing claims\n",
        "    X_evidence: a numpy array of shape (num_documents, max_tokens) representing evidences\n",
        "    max_seq_length: the max token sequence computed with training texts.\n",
        "    \"\"\"\n",
        "\n",
        "    # compute embeddings for terms in the dataset that are out of vocabulary and add them\n",
        "    vectorizer.adapt(dataset, columns)\n",
        "\n",
        "    # use the vocabulary of word_to_idx built to convert the claim and the evidences into idx sequences\n",
        "    X_claim, X_evidence = vectorizer.transform(dataset, columns)\n",
        "\n",
        "    # compute max_tokens\n",
        "    if is_training:\n",
        "        max_tokens = int(np.quantile([len(seq) for seq in X_claim+X_evidence], 0.999))\n",
        "    else:\n",
        "        assert max_tokens is not None\n",
        "\n",
        "    # apply padding to idx sequences\n",
        "    X_claim = [seq + [0] * (max_tokens - len(seq)) for seq in X_claim]\n",
        "    X_evidence = [seq + [0] * (max_tokens - len(seq)) for seq in X_evidence]\n",
        "    X_claim = np.stack([seq[:max_tokens] for seq in X_claim])\n",
        "    X_evidence = np.stack([seq[:max_tokens] for seq in X_evidence])\n",
        "\n",
        "\n",
        "    if is_training:\n",
        "        return X_claim, X_evidence, max_tokens\n",
        "    else:\n",
        "        return X_claim, X_evidence\n",
        "\n",
        "\n",
        "def encode_target(target_series):\n",
        "    \"\"\"\n",
        "    Encodes the target column of the dataset\n",
        "    \"\"\"\n",
        "    return target_series.apply(lambda x: 1 if x == 1 else 0)"
      ],
      "metadata": {
        "id": "TXHVDz2XWUiK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the vectorizer\n",
        "embedding_dim = 300\n",
        "max_tokens = 118 # Determined from the training samples when encoding them\n",
        "PATH_TO_GLOVE = './drive/MyDrive/glove'\n",
        "\n",
        "vectorizer = TextVectorizer(embedding_dim=embedding_dim, embedding_folder=PATH_TO_GLOVE) # Remove the embedding_folder and it will download from standford but sometimes the website is down in which case the glove embeddings saved in google drive are used\n",
        "\n",
        "input_columns = [\"Evidence\", \"Claim\"]\n",
        "target_column = \"label\"\n",
        "\n",
        "# Encoding the test data\n",
        "print(\"\\nTEST SET:\")\n",
        "X_claim_test, X_evidence_test = encode_input(test_data, columns=input_columns, vectorizer=vectorizer, max_tokens=max_tokens) # Encoder the testing data\n",
        "\n",
        "print(\"\\nEmbedding matrix shape: {}\".format(vectorizer.embedding_matrix.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CezwHqDCWXum",
        "outputId": "a7305f21-362c-4bb7-96d5-d9293a5bf010"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating glove vocabulary...\n",
            "./drive/MyDrive/glove/glove.6B.300d.txt\n",
            "\n",
            "TEST SET:\n",
            "Generated embeddings for 6 OOV words.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting data into idx sequences...: 100%|██████████| 50/50 [00:00<00:00, 1749.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding matrix shape: (400008, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_path_2 = './drive/MyDrive/'\n",
        "model_base = tf.keras.models.load_model(os.path.join(models_path_2, \"bi_lstm_pooling\"))"
      ],
      "metadata": {
        "id": "aYmTqFRBXxRE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_model(model, X_claim_test, X_evidence_test, file_name='Group_16_B.csv'):\n",
        "    \"\"\"\n",
        "    Show classification report using model to predict output on X_test,\n",
        "    write predictions to a CSV file, and count the number of predictions as 1s and 0s.\n",
        "    \"\"\"\n",
        "    # Predicting the outputs\n",
        "    y_pred = model.predict({\"claim\": X_claim_test, \"evidence\": X_evidence_test})\n",
        "    y_pred = [1 if y > 0.5 else 0 for y in y_pred]\n",
        "\n",
        "    # Writing predictions to a CSV file\n",
        "    with open(file_name, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['prediction'])  # Writing the header\n",
        "        for pred in y_pred:\n",
        "            writer.writerow([pred])  # Writing each prediction on a new row\n",
        "\n",
        "    # Counting the number of 1s and 0s\n",
        "    counts = Counter(y_pred)\n",
        "    print(f\"Number of 1s (SUPPORTED): {counts[1]}\")\n",
        "    print(f\"Number of 0s (REFUTED): {counts[0]}\")\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "0qj7nkY7YOzX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = evaluate_model(model_base,X_claim_test,X_evidence_test,\"predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m_yA5IEYmO_",
        "outputId": "a90f4649-e996-4019-a0e4-860ea0a4d484"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 10s 363ms/step\n",
            "Number of 1s (SUPPORTED): 15\n",
            "Number of 0s (REFUTED): 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_test = encode_target(test_data[target_column])"
      ],
      "metadata": {
        "id": "5fcvMFfAYsep"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# # Calculate Accuracy\n",
        "# accuracy = accuracy_score(y_test, predictions_test)\n",
        "# print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# # Calculate Precision\n",
        "# precision = precision_score(y_test, predictions_test)\n",
        "# print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "# # Calculate Recall\n",
        "# recall = recall_score(y_test, predictions_test)\n",
        "# print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# # Calculate F1 Score\n",
        "# f1 = f1_score(y_test, predictions_test)\n",
        "# print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvQ6S7r7Y1cy",
        "outputId": "946e8800-3c6d-4899-f357-0899b42d4239"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.92\n",
            "Precision: 0.87\n",
            "Recall: 0.87\n",
            "F1 Score: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7L1D3LtwZB3e"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}