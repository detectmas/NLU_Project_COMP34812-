{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxh8wQZ3aqWP",
        "outputId": "6f4a910f-fbbb-4b09-d7a1-bda42c9d527a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, typeguard, tensorflow-addons, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5 tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8HZ9J3-hxRP",
        "outputId": "ccd5403c-944d-47dc-8ac9-ff89ac4d127c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import json\n",
        "import glob\n",
        "from urllib import request\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from functools import reduce\n",
        "\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate, Dense, Dropout, GlobalAveragePooling1D, Bidirectional, Masking\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNXFt2CAatAr",
        "outputId": "8168708a-1aed-410e-933c-b46ae11cf423"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove = './drive/MyDrive/glove'"
      ],
      "metadata": {
        "id": "FFxtDsg8iGaz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_data = pd.read_csv('train.csv')\n",
        "dev_data = pd.read_csv('dev.csv') # dev_data but for now naming it to trial_data\n",
        "trial_data = pd.read_csv('test.csv')\n",
        "text = train_data['Evidence'].iloc[78]"
      ],
      "metadata": {
        "id": "91Wxemm6b8xW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Split the data into training and validation sets (80/20 split)\n",
        "# train_data, dev_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Display the shape of the training and validation data\n",
        "# train_data.shape, dev_data.shape,trial_data.shape"
      ],
      "metadata": {
        "id": "3lgrpwLF0p6n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze the claim and evidence text non-alpha and non-numeric to understand what kind of preprocessing is needed\n",
        "claim_words = [word for sentence in train_data['Claim'] for word in nltk.word_tokenize(sentence) if not word.isalpha() and not word.isnumeric()]\n",
        "evidence_words = [word for sentence in train_data['Evidence'] for word in nltk.word_tokenize(sentence) if not word.isalpha() and not word.isnumeric()]\n",
        "\n",
        "claim_words_occurences = dict(zip(*np.unique(claim_words, return_counts=True)))\n",
        "evidence_words_occurences = dict(zip(*np.unique(evidence_words, return_counts=True)))"
      ],
      "metadata": {
        "id": "Hn2DvHI0c0GB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile(r'[/(){}\\[\\]\\|@`\\']')\n",
        "REMOVE_BEGINNING_EVIDENCES_RE = re.compile(r'\\b[0-9]{1,}')\n",
        "REMOVE_REF_EVIDENCE_RE = re.compile(r'\\[REF|ref\\]?\\.?')\n",
        "SPLIT_COMPOUND_RE = re.compile(r'\\w+(-)\\w+')\n",
        "GOOD_SYMBOLS_RE = re.compile(r'[^0-9a-z \\.;]')\n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "def remove_beginning_evidence(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes the beginning evidence from the text.\n",
        "    Example:\n",
        "    Input: '1. I really like New York city'\n",
        "    Output: 'I really like New York city'\n",
        "    \"\"\"\n",
        "    return REMOVE_BEGINNING_EVIDENCES_RE.sub('', text)\n",
        "\n",
        "def split_compound_words(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Splits compound words in the text.\n",
        "    Example:\n",
        "    Input: 'I really like Winston-Salem city'\n",
        "    Output: 'I really like Winston Salem city'\n",
        "    \"\"\"\n",
        "    return SPLIT_COMPOUND_RE.sub(' ', text)\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def remove_reference_markers(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove [REF] or [REF at the at of evidence texts\n",
        "    \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        return REMOVE_REF_EVIDENCE_RE.sub('', text)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "    return text.strip()\n",
        "\n",
        "PREPROCESSING_PIPELINE_CLAIM = [\n",
        "                          lower,\n",
        "                          replace_special_characters,\n",
        "                          split_compound_words,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "PREPROCESSING_PIPELINE_EVIDENCE = [\n",
        "                          remove_reference_markers,\n",
        "                          remove_beginning_evidence,\n",
        "                          lower,\n",
        "                          replace_special_characters,\n",
        "                          split_compound_words,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "\n",
        "def text_prepare(text, filter_methods):\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "print('Pre-processing text...')\n",
        "\n",
        "print()\n",
        "print('[Debug] Before:\\n{}'.format(train_data[['Claim', 'Evidence']][:3]))\n",
        "print()\n",
        "\n",
        "# Replace each sentence with its pre-processed version\n",
        "train_data['Claim'] = train_data['Claim'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_CLAIM))\n",
        "train_data['Evidence'] = train_data['Evidence'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_EVIDENCE))\n",
        "\n",
        "dev_data['Claim'] = dev_data['Claim'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_CLAIM))\n",
        "dev_data['Evidence'] = dev_data['Evidence'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_EVIDENCE))\n",
        "\n",
        "trial_data['Claim'] = trial_data['Claim'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_CLAIM))\n",
        "trial_data['Evidence'] = trial_data['Evidence'].apply(lambda txt: text_prepare(txt, PREPROCESSING_PIPELINE_EVIDENCE))\n",
        "\n",
        "print('[Debug] After:\\n{}'.format(train_data[['Claim', 'Evidence']][:3]))\n",
        "print()\n",
        "\n",
        "print(\"Pre-processing completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvsVF8Vac4LI",
        "outputId": "85e00fdc-f21c-44e3-b8f4-3cb2093855f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-processing text...\n",
            "\n",
            "[Debug] Before:\n",
            "                                         Claim  \\\n",
            "0  We should legalize the growing of coca leaf   \n",
            "1       We should ban trans fats usage in food   \n",
            "2              We should legalize prostitution   \n",
            "\n",
            "                                            Evidence  \n",
            "0  Robert W. Sweet, a federal judge, strongly agr...  \n",
            "1  The net increase in LDL/HDL ratio with trans f...  \n",
            "2  Pertaining to health, safety and services, the...  \n",
            "\n",
            "[Debug] After:\n",
            "                                         Claim  \\\n",
            "0  we should legalize the growing of coca leaf   \n",
            "1       we should ban trans fats usage in food   \n",
            "2              we should legalize prostitution   \n",
            "\n",
            "                                            Evidence  \n",
            "0  robert w. sweet a federal judge strongly agree...  \n",
            "1  the net increase in ldl hdl ratio with trans f...  \n",
            "2  pertaining to health safety and services the r...  \n",
            "\n",
            "Pre-processing completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NotAdaptedError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class TextVectorizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        glove_url=\"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
        "        embedding_dim=100,\n",
        "        embedding_folder=\"glove\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This class parses the GloVe embeddings, the input documents are expected\n",
        "        to be in the form of a list of lists.\n",
        "        [[\"word1\", \"word2\", ...], [\"word1\", \"word2\", ...], ...]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        glove_url : The url of the GloVe embeddings.\n",
        "        embedding_dim : The dimension of the embeddings (pick one of 50, 100, 200, 300).\n",
        "        embedding_folder : folder where the embedding will be downloaded\n",
        "        \"\"\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.download_glove_if_needed(\n",
        "            glove_url=glove_url, embedding_folder=embedding_folder\n",
        "        )\n",
        "\n",
        "        # create the embeddings vocabulary\n",
        "        self.vocabulary = self.parse_glove(embedding_folder)\n",
        "\n",
        "    def download_glove_if_needed(self, glove_url, embedding_folder):\n",
        "        \"\"\"\n",
        "        Downloads the glove embeddings from the internet\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        glove_url : The url of the GloVe embeddings.\n",
        "        embedding_folder: folder where the embedding will be downloaded\n",
        "        \"\"\"\n",
        "        # create embedding folder if it does not exist\n",
        "        if not os.path.exists(embedding_folder):\n",
        "            os.makedirs(embedding_folder)\n",
        "\n",
        "        # extract the embedding if it is not extracted\n",
        "        if not glob.glob(\n",
        "            os.path.join(embedding_folder, \"**/glove*.txt\"), recursive=True\n",
        "        ):\n",
        "\n",
        "            # download the embedding if it does not exist\n",
        "            embedding_zip = os.path.join(embedding_folder, glove_url.split(\"/\")[-1])\n",
        "            print(embedding_zip)\n",
        "            if not os.path.exists(embedding_zip):\n",
        "                print(\"Downloading the GloVe embeddings...\")\n",
        "                request.urlretrieve(glove_url, embedding_zip)\n",
        "                print(\"Successful download!\")\n",
        "\n",
        "            # extract the embedding\n",
        "            print(\"Extracting the embeddings...\")\n",
        "            with zipfile.ZipFile(embedding_zip, \"r\") as zip_ref:\n",
        "                zip_ref.extractall(embedding_folder)\n",
        "                print(\"Successfully extracted the embeddings!\")\n",
        "            os.remove(embedding_zip)\n",
        "\n",
        "    def parse_glove(self, embedding_folder):\n",
        "        \"\"\"\n",
        "        Parses the GloVe embeddings from their files, filling the vocabulary.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding_folder : folder where the embedding files are stored\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dictionary representing the vocabulary from the embeddings\n",
        "        \"\"\"\n",
        "        print(\"Creating glove vocabulary...\")\n",
        "        vocabulary = {\"<pad>\": np.zeros(self.embedding_dim)}\n",
        "        embedding_file = os.path.join(\n",
        "            embedding_folder, \"glove.6B.\" + str(self.embedding_dim) + \"d.txt\"\n",
        "        )\n",
        "        print(embedding_file)\n",
        "        with open(embedding_file, encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                vocabulary[word] = coefs\n",
        "        return vocabulary\n",
        "    def adapt(self, dataset, columns):\n",
        "        \"\"\"\n",
        "        Computes the OOV words for a single data split, and adds them to the vocabulary and recreate the dictionary of index encodings.\n",
        "        Then build the embedding matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : The data split (might be training set, validation set, or test set).\n",
        "        columns : The columns to be adapted.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        The embedding matrix of shape (vocabulary_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        # create a set containing words from the documents in a given data split\n",
        "        words = {word for column in columns for sentence in dataset[column] for word in nltk.word_tokenize(sentence)}\n",
        "        oov_words = words - self.vocabulary.keys()\n",
        "\n",
        "        # add the OOV words to the vocabulary giving them a random encoding\n",
        "        for word in oov_words:\n",
        "            self.vocabulary[word] = np.random.uniform(-1, 1, size=self.embedding_dim)\n",
        "\n",
        "        # create the dictionary of index encodings for the words in the embedding vocabulary (idx 0 is reserved for padding)\n",
        "        self.word_to_idx = {word: i for i, word in enumerate(self.vocabulary.keys())}\n",
        "        self.idx_to_word = {i: word for i, word in enumerate(self.vocabulary.keys())}\n",
        "\n",
        "        # the embedding matrix shape will be (vocabulary_size, embedding_dim)\n",
        "        self.embedding_matrix = np.array(list(self.vocabulary.values()))\n",
        "        print(f\"Generated embeddings for {len(oov_words)} OOV words.\")\n",
        "\n",
        "    def transform(self, dataset, columns):\n",
        "        \"\"\"\n",
        "        Transform the data into the input structure for the training. This method should be used always after the adapt method.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : The data split (might be training set, validation set, or test set).\n",
        "        columns : The columns to be transformed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Pair of docuemnts into idx sequences\n",
        "        \"\"\"\n",
        "        X_claim, X_evidence = [], []\n",
        "        for _, row in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Converting data into idx sequences...\"):\n",
        "            X_claim.append(self._transform_document_to_encoding(row[\"Claim\"]))\n",
        "            X_evidence.append(self._transform_document_to_encoding(row[\"Evidence\"]))\n",
        "        return X_claim, X_evidence\n",
        "\n",
        "\n",
        "    def _transform_document_to_encoding(self, document):\n",
        "        \"\"\"\n",
        "        Transforms a single document to a list of word encodings.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        document : The document to be transformed.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List of word encodings\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return [self.word_to_idx[word] for word in nltk.word_tokenize(document)]\n",
        "        except KeyError:\n",
        "            raise NotAdaptedError(\n",
        "                f\"The whole document is not in the vocabulary. Please adapt the vocabulary first.\"\n",
        "            )"
      ],
      "metadata": {
        "id": "Z9K_7QUic8KF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input(dataset, columns, vectorizer, is_training=False, max_tokens=None):\n",
        "    \"\"\"\n",
        "    Convert the text into a given dataset split into idx sequeces.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : The data split (might be training set, validation set, or test set).\n",
        "    columns : The columns to be converted.\n",
        "    vectorizer : The vectorizer to be used.\n",
        "    is_training : Whether input texts are from the training split or not\n",
        "    max_tokens : The max token sequence previously computed with\n",
        "\n",
        "    Return\n",
        "    ---------\n",
        "    X_claim: a numpy array of shape (num_documents, max_tokens) representing claims\n",
        "    X_evidence: a numpy array of shape (num_documents, max_tokens) representing evidences\n",
        "    max_seq_length: the max token sequence computed with training texts.\n",
        "    \"\"\"\n",
        "\n",
        "    # compute embeddings for terms in the dataset that are out of vocabulary and add them\n",
        "    vectorizer.adapt(dataset, columns)\n",
        "\n",
        "    # use the vocabulary of word_to_idx built to convert the claim and the evidences into idx sequences\n",
        "    X_claim, X_evidence = vectorizer.transform(dataset, columns)\n",
        "\n",
        "    # compute max_tokens\n",
        "    if is_training:\n",
        "        max_tokens = int(np.quantile([len(seq) for seq in X_claim+X_evidence], 0.999))\n",
        "    else:\n",
        "        assert max_tokens is not None\n",
        "\n",
        "    # apply padding to idx sequences\n",
        "    X_claim = [seq + [0] * (max_tokens - len(seq)) for seq in X_claim]\n",
        "    X_evidence = [seq + [0] * (max_tokens - len(seq)) for seq in X_evidence]\n",
        "    X_claim = np.stack([seq[:max_tokens] for seq in X_claim])\n",
        "    X_evidence = np.stack([seq[:max_tokens] for seq in X_evidence])\n",
        "\n",
        "\n",
        "    if is_training:\n",
        "        return X_claim, X_evidence, max_tokens\n",
        "    else:\n",
        "        return X_claim, X_evidence\n",
        "\n",
        "\n",
        "def encode_target(target_series):\n",
        "    \"\"\"\n",
        "    Encodes the target column of the dataset\n",
        "    \"\"\"\n",
        "    return target_series.apply(lambda x: 1 if x == 1 else 0)"
      ],
      "metadata": {
        "id": "hO02Xcqbc-0M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the vectorizer\n",
        "embedding_dim = 300\n",
        "vectorizer = TextVectorizer(embedding_dim=embedding_dim, embedding_folder=path_to_glove)\n",
        "\n",
        "input_columns = [\"Evidence\", \"Claim\"]\n",
        "target_column = \"label\"\n",
        "\n",
        "# Train\n",
        "print(\"\\nTRAINING SET:\")\n",
        "X_claim_train, X_evidence_train, max_tokens = encode_input(train_data, columns=input_columns, vectorizer=vectorizer, is_training=True)\n",
        "y_train = encode_target(train_data[target_column])\n",
        "print(\"Max token sequence: {}\".format(max_tokens))\n",
        "\n",
        "print('X claim train shape: ', X_claim_train.shape)\n",
        "print('X evidence train shape: ', X_evidence_train.shape)\n",
        "print('y train shape: ', y_train.shape)\n",
        "\n",
        "# Validation\n",
        "print(\"\\DEVELOPMENT SET:\")\n",
        "X_claim_val, X_evidence_val = encode_input(dev_data, columns=input_columns, vectorizer=vectorizer, max_tokens=max_tokens)\n",
        "y_val = encode_target(dev_data[target_column])\n",
        "\n",
        "print(\"X claim dev shape: \", X_claim_val.shape)\n",
        "print(\"X evidence dev shape: \", X_evidence_val.shape)\n",
        "print(\"y dev shape: \", y_val.shape)\n",
        "\n",
        "# Test\n",
        "print(\"\\nTEST SET:\")\n",
        "X_claim_test, X_evidence_test = encode_input(trial_data, columns=input_columns, vectorizer=vectorizer, max_tokens=max_tokens)\n",
        "# y_test = encode_target(trial_data[target_column])\n",
        "\n",
        "print(\"X claim test shape: \", X_claim_test.shape)\n",
        "print(\"X evidence test shape: \", X_evidence_test.shape)\n",
        "# print(\"y trial shape: \", y_test.shape)\n",
        "\n",
        "print(\"\\nEmbedding matrix shape: {}\".format(vectorizer.embedding_matrix.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4IjW3OcdD-x",
        "outputId": "44e2108e-bd22-49de-bcc3-2c53cb5b3ec2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating glove vocabulary...\n",
            "./drive/MyDrive/glove/glove.6B.300d.txt\n",
            "\n",
            "TRAINING SET:\n",
            "Generated embeddings for 3140 OOV words.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting data into idx sequences...: 100%|██████████| 23702/23702 [00:09<00:00, 2547.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token sequence: 118\n",
            "X claim train shape:  (23702, 118)\n",
            "X evidence train shape:  (23702, 118)\n",
            "y train shape:  (23702,)\n",
            "\\DEVELOPMENT SET:\n",
            "Generated embeddings for 759 OOV words.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting data into idx sequences...: 100%|██████████| 5926/5926 [00:02<00:00, 2580.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X claim dev shape:  (5926, 118)\n",
            "X evidence dev shape:  (5926, 118)\n",
            "y dev shape:  (5926,)\n",
            "\n",
            "TEST SET:\n",
            "Generated embeddings for 489 OOV words.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting data into idx sequences...: 100%|██████████| 4691/4691 [00:01<00:00, 2371.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X claim test shape:  (4691, 118)\n",
            "X evidence test shape:  (4691, 118)\n",
            "\n",
            "Embedding matrix shape: (404390, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Configure GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Additional setup: Check if GPUs are available and memory growth is set\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPUs found\")\n",
        "\n",
        "\n",
        "def build_model(max_tokens, embedding_matrix, choice_info, compile_info):\n",
        "    \"\"\"\n",
        "    Use functional API of tf.keras to build the model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_tokens : The max token sequence previously computed\n",
        "    embedding_matrix : The embedding matrix to be used.\n",
        "    choice_info : dictionary containing choice information about embedding and merging mode, and other choice points\n",
        "    compile_info: dictionary containing compile information\n",
        "\n",
        "    Return\n",
        "    ----------\n",
        "    model: the built keras functional model\n",
        "    \"\"\"\n",
        "\n",
        "    assert choice_info[\"merging_mode\"] in [\"concat\", \"add\", \"mean\"]\n",
        "    assert choice_info[\"embedding_mode\"] in [\"bi_lstm\"]\n",
        "    assert isinstance(choice_info[\"dropout_rate\"], float)\n",
        "    assert(all(isinstance(item, int) for item in choice_info[\"classification_units\"]))\n",
        "\n",
        "    regularizer = tf.keras.regularizers.l2(0.01)\n",
        "\n",
        "    # embedding of claim and evidence layers\n",
        "    claim_input = Input(shape=(max_tokens), name=\"claim\")\n",
        "    claim_masking = Masking(mask_value=0, name=\"claim_masking\")(claim_input)\n",
        "    evidence_input = Input(shape=(max_tokens), name=\"evidence\")\n",
        "    evidence_masking = Masking(mask_value=0, name=\"evidence_masking\")(evidence_input)\n",
        "\n",
        "    claim_embedding = Embedding(input_dim=embedding_matrix.shape[0],  # vocab size\n",
        "                                output_dim=embedding_matrix.shape[1], # embedding dim\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=False,\n",
        "                                mask_zero=True,\n",
        "                                name=\"claim_embedding\")(claim_masking)\n",
        "    evidence_embedding = Embedding(input_dim=embedding_matrix.shape[0],  # vocab size\n",
        "                                   output_dim=embedding_matrix.shape[1], # embedding dim\n",
        "                                   weights=[embedding_matrix],\n",
        "                                   trainable=False,\n",
        "                                   mask_zero=True,\n",
        "                                   name=\"evidence_embedding\")(evidence_masking)\n",
        "\n",
        "    if choice_info[\"embedding_mode\"] == \"bi_lstm\":\n",
        "        claim_encoding = Bidirectional(LSTM(units=embedding_matrix.shape[1], return_sequences=True), merge_mode='concat', name=\"claim_encoding\")(claim_embedding)\n",
        "        evidence_encoding = Bidirectional(LSTM(units=embedding_matrix.shape[1], return_sequences=True), merge_mode='concat', name=\"evidence_encoding\")(evidence_embedding)\n",
        "        claim_encoding = GlobalAveragePooling1D(name=\"claim_encoding_avg\")(claim_encoding)\n",
        "        evidence_encoding = GlobalAveragePooling1D(name=\"evidence_encoding_avg\")(evidence_encoding)\n",
        "\n",
        "    # merging layer\n",
        "    if choice_info[\"merging_mode\"] == \"concat\":\n",
        "        merged = tf.keras.layers.concatenate([claim_encoding, evidence_encoding])\n",
        "    elif choice_info[\"merging_mode\"] == \"add\":\n",
        "        merged = tf.keras.layers.add([claim_encoding, evidence_encoding])\n",
        "    elif choice_info[\"merging_mode\"] == \"mean\":\n",
        "        merged = tf.keras.layers.average([claim_encoding, evidence_encoding])\n",
        "\n",
        "    classification_input = merged\n",
        "\n",
        "    # classification layers\n",
        "    for i, units in enumerate(choice_info[\"classification_units\"]):\n",
        "        dense = Dense(units=units, activation=\"relu\", kernel_regularizer=regularizer, name=f\"classification_{i+1}\")(classification_input)\n",
        "        classification_input = Dropout(rate=choice_info[\"dropout_rate\"], name=f\"dropout_{i+1}\")(dense)\n",
        "    dense = Dense(units=1, activation=\"sigmoid\", kernel_regularizer=regularizer, name=\"output\")(classification_input)\n",
        "\n",
        "    model = Model(inputs=[claim_input, evidence_input], outputs=dense)\n",
        "    model.compile(**compile_info)\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqqNnpVedFaU",
        "outputId": "e803ebb3-ecdd-42ce-8f71-edd317d5e146"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.round(y_pred)\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_state()\n",
        "        self.recall.reset_state()\n"
      ],
      "metadata": {
        "id": "oJsG1NG55ejN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs_tuning = 30\n",
        "# batch_size_tuning = 64\n",
        "\n",
        "# compile_info_tuning = {\n",
        "#     'optimizer': tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
        "#     'loss': 'binary_crossentropy',\n",
        "#     'metrics': [F1Score()],\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "# tuner = kt.Hyperband(lambda hp: build_model(max_tokens,\n",
        "#                                             vectorizer.embedding_matrix, {\n",
        "#                                                 \"embedding_mode\": hp.Choice('embedding_mode', [\"bi_lstm\"]),\n",
        "#                                                 \"merging_mode\": hp.Choice('merging_mode', [\"concat\", \"add\", \"mean\"]),\n",
        "#                                                 \"dropout_rate\": hp.Choice('dropout_rate', [0.2, 0.3, 0.4]),\n",
        "#                                                 \"classification_units\": json.loads(hp.Choice('classification_units', [json.dumps(units) for l in range(1, 3) for units in itertools.permutations([64, 128, 256], l)])),\n",
        "#                                                 },\n",
        "#                                              compile_info_tuning),\n",
        "#                      objective=kt.Objective(\"val_f1_score\", direction=\"max\"),\n",
        "#                      max_epochs=epochs_tuning,\n",
        "#                      directory=\"tuner\",\n",
        "#                      project_name=\"tuner_fact_checking\")\n",
        "\n",
        "\n",
        "# tuner.search({\"claim\": X_claim_train, \"evidence\": X_evidence_train}, y_train, epochs=epochs_tuning, batch_size=batch_size_tuning, validation_data=({\"claim\": X_claim_val, \"evidence\": X_evidence_val}, y_val))\n",
        "# best_hps = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "# print(f\"The hyperparameter search is complete.\\n\"\n",
        "#       f\"The optimal sentence embedding mode is {best_hps.get('embedding_mode')}.\\n\"\n",
        "#       f\"The optimal sentence merging mode is {best_hps.get('merging_mode')}.\\n\"\n",
        "#       f\"The optimal rate for Dropout layer is {best_hps.get('dropout_rate')}.\\n\"\n",
        "#       f\"The optimal classification units are {best_hps.get('classification_units')}.\")"
      ],
      "metadata": {
        "id": "-4GKvxmgjwM1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Best values determine from hyperparameter tuning.\n",
        "choice_info = {\n",
        "    \"embedding_mode\": \"bi_lstm\",\n",
        "    \"merging_mode\": \"concat\",\n",
        "    \"dropout_rate\" : 0.3,\n",
        "    \"classification_units\": [256, 64]\n",
        "}\n",
        "\n",
        "compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'binary_crossentropy',\n",
        "    'metrics': 'accuracy',\n",
        "}\n",
        "\n",
        "model_base = build_model(max_tokens, vectorizer.embedding_matrix, choice_info, compile_info)\n",
        "model_base.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IQAA4Fdj3ak",
        "outputId": "71514a55-f6ff-4666-af85-d8136598af38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " claim (InputLayer)          [(None, 118)]                0         []                            \n",
            "                                                                                                  \n",
            " evidence (InputLayer)       [(None, 118)]                0         []                            \n",
            "                                                                                                  \n",
            " claim_masking (Masking)     (None, 118)                  0         ['claim[0][0]']               \n",
            "                                                                                                  \n",
            " evidence_masking (Masking)  (None, 118)                  0         ['evidence[0][0]']            \n",
            "                                                                                                  \n",
            " claim_embedding (Embedding  (None, 118, 300)             1213170   ['claim_masking[0][0]']       \n",
            " )                                                        00                                      \n",
            "                                                                                                  \n",
            " evidence_embedding (Embedd  (None, 118, 300)             1213170   ['evidence_masking[0][0]']    \n",
            " ing)                                                     00                                      \n",
            "                                                                                                  \n",
            " claim_encoding (Bidirectio  (None, 118, 600)             1442400   ['claim_embedding[0][0]']     \n",
            " nal)                                                                                             \n",
            "                                                                                                  \n",
            " evidence_encoding (Bidirec  (None, 118, 600)             1442400   ['evidence_embedding[0][0]']  \n",
            " tional)                                                                                          \n",
            "                                                                                                  \n",
            " claim_encoding_avg (Global  (None, 600)                  0         ['claim_encoding[0][0]']      \n",
            " AveragePooling1D)                                                                                \n",
            "                                                                                                  \n",
            " evidence_encoding_avg (Glo  (None, 600)                  0         ['evidence_encoding[0][0]']   \n",
            " balAveragePooling1D)                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 1200)                 0         ['claim_encoding_avg[0][0]',  \n",
            "                                                                     'evidence_encoding_avg[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " classification_1 (Dense)    (None, 256)                  307456    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 256)                  0         ['classification_1[0][0]']    \n",
            "                                                                                                  \n",
            " classification_2 (Dense)    (None, 64)                   16448     ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 64)                   0         ['classification_2[0][0]']    \n",
            "                                                                                                  \n",
            " output (Dense)              (None, 1)                    65        ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 245842769 (937.82 MB)\n",
            "Trainable params: 3208769 (12.24 MB)\n",
            "Non-trainable params: 242634000 (925.58 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_train = 64\n",
        "epochs_training = 50\n",
        "callbacks_training = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode=\"max\", restore_best_weights=True)]\n",
        "\n",
        "history_base = model_base.fit({\"claim\": X_claim_train, \"evidence\": X_evidence_train}, y_train, epochs=epochs_training, batch_size=batch_size_train,  validation_data=({\"claim\": X_claim_val, \"evidence\": X_evidence_val}, y_val), callbacks=callbacks_training)\n",
        "history_base = history_base.history"
      ],
      "metadata": {
        "id": "w-llPJrUhARz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b6e9d2-8bfc-4474-f34f-26cc4e9351f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "371/371 [==============================] - 32s 36ms/step - loss: 0.8425 - accuracy: 0.7901 - val_loss: 0.4301 - val_accuracy: 0.8139\n",
            "Epoch 2/50\n",
            "371/371 [==============================] - 8s 22ms/step - loss: 0.4308 - accuracy: 0.8174 - val_loss: 0.4263 - val_accuracy: 0.8112\n",
            "Epoch 3/50\n",
            "371/371 [==============================] - 9s 23ms/step - loss: 0.4077 - accuracy: 0.8280 - val_loss: 0.4052 - val_accuracy: 0.8216\n",
            "Epoch 4/50\n",
            "371/371 [==============================] - 8s 21ms/step - loss: 0.3918 - accuracy: 0.8384 - val_loss: 0.3993 - val_accuracy: 0.8314\n",
            "Epoch 5/50\n",
            "371/371 [==============================] - 8s 21ms/step - loss: 0.3742 - accuracy: 0.8482 - val_loss: 0.3989 - val_accuracy: 0.8286\n",
            "Epoch 6/50\n",
            "371/371 [==============================] - 8s 22ms/step - loss: 0.3487 - accuracy: 0.8639 - val_loss: 0.4022 - val_accuracy: 0.8242\n",
            "Epoch 7/50\n",
            "371/371 [==============================] - 8s 22ms/step - loss: 0.3216 - accuracy: 0.8811 - val_loss: 0.4131 - val_accuracy: 0.8311\n",
            "Epoch 8/50\n",
            "371/371 [==============================] - 8s 22ms/step - loss: 0.2888 - accuracy: 0.9017 - val_loss: 0.4288 - val_accuracy: 0.8264\n",
            "Epoch 9/50\n",
            "371/371 [==============================] - 9s 24ms/step - loss: 0.2598 - accuracy: 0.9173 - val_loss: 0.4819 - val_accuracy: 0.8223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save models\n",
        "# models_path = \"models\"\n",
        "models_path_2 = './drive/MyDrive/'\n",
        "model_base.save(os.path.join(models_path_2, \"bi_lstm_pooling\"))"
      ],
      "metadata": {
        "id": "u0c1MXyutJyr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the history.history dict to a pandas DataFrame:\n",
        "path_model_history = os.path.join(models_path_2,'history')\n",
        "# Now, create the directory if it doesn't exist\n",
        "os.makedirs(path_model_history, exist_ok=True)  # This will create the directory if it does not exist"
      ],
      "metadata": {
        "id": "O3uLstcbtOK-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_history_base = pd.DataFrame(history_base)\n",
        "with open(os.path.join(path_model_history, \"history_base.csv\"), mode=\"w\") as file:\n",
        "    df_history_base.to_csv(file)"
      ],
      "metadata": {
        "id": "pLC3tCovtRfq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reload model\n",
        "model_base = tf.keras.models.load_model(os.path.join(models_path_2, \"bi_lstm_pooling\"))\n",
        "# Restore history\n",
        "cols = ['loss','accuracy','val_loss','val_accuracy']\n",
        "path_model_history = os.path.join(models_path_2, 'history')\n",
        "history_base = pd.read_csv(os.path.join(path_model_history, \"history_base.csv\"), usecols=cols) # Use this to plot graph?"
      ],
      "metadata": {
        "id": "ACsfYyxVTxh-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_model(model, X_claim_test, X_evidence_test, file_name='Group_16_B.csv'):\n",
        "    \"\"\"\n",
        "    Show classification report using model to predict output on X_test,\n",
        "    write predictions to a CSV file, and count the number of predictions as 1s and 0s.\n",
        "    \"\"\"\n",
        "    # Predicting the outputs\n",
        "    y_pred = model.predict({\"claim\": X_claim_test, \"evidence\": X_evidence_test})\n",
        "    y_pred = [1 if y > 0.5 else 0 for y in y_pred]\n",
        "\n",
        "    # Writing predictions to a CSV file\n",
        "    with open(file_name, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['prediction'])  # Writing the header\n",
        "        for pred in y_pred:\n",
        "            writer.writerow([pred])  # Writing each prediction on a new row\n",
        "\n",
        "    # Counting the number of 1s and 0s\n",
        "    counts = Counter(y_pred)\n",
        "    print(f\"Number of 1s (SUPPORTED): {counts[1]}\")\n",
        "    print(f\"Number of 0s (REFUTED): {counts[0]}\")\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "uP5XAClbmlLD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = evaluate_model(model_base, X_claim_val, X_evidence_val,'validation_preds.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsuOuDvztGLP",
        "outputId": "655ea892-aa8c-4b2c-b3a4-0ec5dc18ed46"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "186/186 [==============================] - 7s 7ms/step\n",
            "Number of 1s (SUPPORTED): 1520\n",
            "Number of 0s (REFUTED): 4406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = evaluate_model(model_base,X_claim_test,X_evidence_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8eVvQ3yvpFu",
        "outputId": "25ece9a5-6c63-49d3-8a8f-c9bc7f5ae402"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147/147 [==============================] - 1s 8ms/step\n",
            "Number of 1s (SUPPORTED): 2365\n",
            "Number of 0s (REFUTED): 2326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Calculate Precision\n",
        "precision = precision_score(y_val, predictions, average='macro')\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "# Calculate Recall\n",
        "recall = recall_score(y_val, predictions, average='macro')\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1 = f1_score(y_val, predictions, average='macro')\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1Il2vnVwO50",
        "outputId": "68fbd76f-d354-4f2a-fa35-f10e43c20eba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n",
            "Precision: 0.79\n",
            "Recall: 0.78\n",
            "F1 Score: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1_gxwB6xVw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}